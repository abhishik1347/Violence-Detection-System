{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to load the violence dataset (you need to adjust the path)\n",
    "def load_violence_dataset(path):\n",
    "    videos = []\n",
    "    labels = []\n",
    "    for folder in os.listdir(path):\n",
    "        folder_path = os.path.join(path, folder)\n",
    "        print(f\"Processing folder: {folder_path}\")\n",
    "        if os.path.isdir(folder_path):\n",
    "            label = 1 if 'violence' in folder.lower() else 0  # 1: Violence, 0: Non-violence\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(('.jpg', '.png')):\n",
    "                    image_path = os.path.join(folder_path, file)\n",
    "                    try:\n",
    "                        image = cv2.imread(image_path)\n",
    "                        if image is None:\n",
    "                            print(f\"Failed to load image {image_path}. Skipping...\")\n",
    "                            continue\n",
    "\n",
    "                        image = cv2.resize(image, (224, 224))  # Resize to the model input size\n",
    "                        videos.append(image)\n",
    "                        labels.append(label)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing image {image_path}: {e}\")\n",
    "                        continue\n",
    "    videos = np.array(videos)\n",
    "    labels = np.array(labels)\n",
    "    return videos, labels\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess_data(images):\n",
    "    images = images.astype('float32') / 255.0\n",
    "    return images\n",
    "\n",
    "# Autoencoder Model\n",
    "def create_autoencoder(input_shape):\n",
    "    # Encoder\n",
    "    input_img = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    # Bottleneck\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    return autoencoder\n",
    "\n",
    "# Compile the model\n",
    "def compile_autoencoder(autoencoder):\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "                        loss='mean_squared_error',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "# Train the autoencoder model\n",
    "def train_autoencoder(autoencoder, train_images, val_images):\n",
    "    batch_size = 8\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = autoencoder.fit(train_images, train_images,  # Autoencoders train to reconstruct the input\n",
    "                              validation_data=(val_images, val_images),\n",
    "                              epochs=15,\n",
    "                              batch_size=batch_size,\n",
    "                              callbacks=[early_stopping])\n",
    "    return history\n",
    "\n",
    "# Test the model and calculate performance metrics\n",
    "def test_autoencoder(autoencoder, test_images, true_labels, threshold=0.02):\n",
    "    reconstructions = autoencoder.predict(test_images)\n",
    "    \n",
    "    # Compute reconstruction error\n",
    "    errors = np.mean(np.abs(reconstructions - test_images), axis=(1, 2, 3))\n",
    "    anomalies = errors > threshold\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, anomalies)\n",
    "    precision = precision_score(true_labels, anomalies)\n",
    "    recall = recall_score(true_labels, anomalies)\n",
    "    f1 = f1_score(true_labels, anomalies)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    \n",
    "    return errors, anomalies, accuracy, precision, recall, f1\n",
    "\n",
    "# Main function to execute the workflow\n",
    "def main():\n",
    "    dataset_path = r'C:\\Users\\abhishik chebrolu\\Downloads\\AINN pro\\archive\\new_violence'  # Update with the actual dataset path\n",
    "    \n",
    "    # Load the dataset and labels\n",
    "    images, labels = load_violence_dataset(dataset_path)\n",
    "    images = preprocess_data(images)\n",
    "    \n",
    "    # Split into training and testing sets (80% training, 20% testing)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.3, random_state=42, stratify=labels)\n",
    "    \n",
    "    # Further split training data into training and validation sets (80% train, 20% validation)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42, stratify=y_train)\n",
    "    \n",
    "    # Create and compile the autoencoder model\n",
    "    input_shape = (224, 224, 3)\n",
    "    autoencoder = create_autoencoder(input_shape)\n",
    "    compile_autoencoder(autoencoder)\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    print(\"Training the autoencoder...\")\n",
    "    history = train_autoencoder(autoencoder, X_train, X_val)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    # Test the autoencoder and calculate accuracy, precision, recall, F1\n",
    "    print(\"Testing the autoencoder...\")\n",
    "    errors, anomalies, accuracy, precision, recall, f1 = test_autoencoder(autoencoder, X_test, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7752 images belonging to 2 classes.\n",
      "Found 3321 images belonging to 2 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhishik chebrolu\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m627s\u001b[0m 3s/step - accuracy: 0.4533 - loss: 0.0633 - val_accuracy: 0.5443 - val_loss: 0.0444\n",
      "Epoch 2/10\n",
      "\u001b[1m  1/242\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:21\u001b[0m 2s/step - accuracy: 0.5656 - loss: 0.0374"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5656 - loss: 0.0374 - val_accuracy: 0.4266 - val_loss: 0.0492\n",
      "Epoch 3/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m631s\u001b[0m 3s/step - accuracy: 0.5057 - loss: 0.0401 - val_accuracy: 0.5622 - val_loss: 0.0407\n",
      "Epoch 4/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5323 - loss: 0.0396 - val_accuracy: 0.4116 - val_loss: 0.0448\n",
      "Epoch 5/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m729s\u001b[0m 3s/step - accuracy: 0.5320 - loss: 0.0357 - val_accuracy: 0.5272 - val_loss: 0.0380\n",
      "Epoch 6/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.5785 - loss: 0.0393 - val_accuracy: 0.4990 - val_loss: 0.0399\n",
      "Epoch 7/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m940s\u001b[0m 4s/step - accuracy: 0.5433 - loss: 0.0335 - val_accuracy: 0.5767 - val_loss: 0.0358\n",
      "Epoch 8/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5678 - loss: 0.0319 - val_accuracy: 0.5016 - val_loss: 0.0417\n",
      "Epoch 9/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m715s\u001b[0m 3s/step - accuracy: 0.5575 - loss: 0.0314 - val_accuracy: 0.5777 - val_loss: 0.0365\n",
      "Epoch 10/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.5485 - loss: 0.0347 - val_accuracy: 0.4762 - val_loss: 0.0416\n",
      "Testing the autoencoder...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer \"functional\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(32, 224, 224, 3) dtype=float32>, <tf.Tensor 'data_1:0' shape=(32, 224, 224, 3) dtype=float32>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 170\u001b[0m\n\u001b[0;32m    167\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 170\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 156\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Testing accuracy\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting the autoencoder...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 156\u001b[0m test_accuracy, precision, recall, f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtest_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Plot training accuracy and testing accuracy\u001b[39;00m\n\u001b[0;32m    159\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[1;32mIn[1], line 60\u001b[0m, in \u001b[0;36mtest_autoencoder\u001b[1;34m(autoencoder, test_generator)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Batch-wise reconstruction and error calculation\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m test_generator:\n\u001b[1;32m---> 60\u001b[0m     reconstructed_batch \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     reconstructions\u001b[38;5;241m.\u001b[39mappend(reconstructed_batch)\n\u001b[0;32m     62\u001b[0m     true_labels\u001b[38;5;241m.\u001b[39mappend(batch)  \u001b[38;5;66;03m# The input batch itself is used as the ground truth\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\input_spec.py:160\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_spec):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input(s),\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but it received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input tensors. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_index, (x, spec) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(inputs, input_spec)):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Layer \"functional\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(32, 224, 224, 3) dtype=float32>, <tf.Tensor 'data_1:0' shape=(32, 224, 224, 3) dtype=float32>]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create Transfer Learning Autoencoder (VGG16 as Encoder)\n",
    "def create_transfer_learning_autoencoder(input_shape):\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    \n",
    "    # Freeze the encoder (VGG16) layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Decoder (ensure correct output size: 224x224x3)\n",
    "    x = base_model.output  # The output from VGG16 will be (7, 7, 512)\n",
    "\n",
    "    # Decoder starts here: reverse the pooling/downsampling\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)  # From (7, 7, 512) to (14, 14, 512)\n",
    "\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)  # From (14, 14, 256) to (28, 28, 256)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)  # From (28, 28, 128) to (56, 56, 128)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)  # From (56, 56, 64) to (112, 112, 64)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)  # From (112, 112, 32) to (224, 224, 32)\n",
    "\n",
    "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)  # Final output shape (224, 224, 3)\n",
    "\n",
    "    # Create autoencoder model\n",
    "    autoencoder = Model(base_model.input, decoded)\n",
    "    return autoencoder\n",
    "\n",
    "# Compile the model\n",
    "def compile_autoencoder(autoencoder):\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "                        loss='mean_squared_error',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "# Test the model and calculate performance metrics\n",
    "def test_autoencoder(autoencoder, test_generator):\n",
    "    reconstructions = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Batch-wise reconstruction and error calculation\n",
    "    for batch in test_generator:\n",
    "        reconstructed_batch = autoencoder.predict(batch)\n",
    "        reconstructions.append(reconstructed_batch)\n",
    "        true_labels.append(batch)  # The input batch itself is used as the ground truth\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    reconstructions = np.concatenate(reconstructions, axis=0)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "    # Compute reconstruction error (Mean Squared Error) for each sample\n",
    "    errors = np.mean(np.abs(reconstructions - true_labels), axis=(1, 2, 3))\n",
    "\n",
    "    # Use a threshold to classify anomalies (assuming high reconstruction error as anomaly)\n",
    "    threshold = 0.02  # You can tune this value based on the dataset\n",
    "    anomalies = errors > threshold\n",
    "\n",
    "    # Test labels (for binary classification task)\n",
    "    filenames = test_generator.filenames\n",
    "    true_class_labels = np.array([1 if 'violence' in filename else 0 for filename in filenames])\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_class_labels, anomalies)\n",
    "    precision = precision_score(true_class_labels, anomalies)\n",
    "    recall = recall_score(true_class_labels, anomalies)\n",
    "    f1 = f1_score(true_class_labels, anomalies)\n",
    "\n",
    "    print(f\"Testing Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(true_class_labels, anomalies)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Main function to execute workflow\n",
    "def main():\n",
    "    dataset_path = r'C:\\Users\\abhishik chebrolu\\Downloads\\AINN pro\\archive\\new_violence'  # Path to your dataset with violence and non-violence frames\n",
    "    input_shape = (224, 224, 3)\n",
    "    \n",
    "    # Create and compile the autoencoder\n",
    "    autoencoder = create_transfer_learning_autoencoder(input_shape)\n",
    "    compile_autoencoder(autoencoder)\n",
    "    \n",
    "    # Use ImageDataGenerator for data loading and augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.2,\n",
    "        validation_split=0.3  # Split 70-30 for training and testing\n",
    "    )\n",
    "    \n",
    "    batch_size = 32\n",
    "    \n",
    "    # Train and validation data generators\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        dataset_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='input',  # For autoencoder, input and target are the same\n",
    "        subset='training'\n",
    "    )\n",
    "    \n",
    "    # Test data generator (30% test split, no augmentation)\n",
    "    test_generator = datagen.flow_from_directory(\n",
    "        dataset_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='input',  # For autoencoder, input and target are the same\n",
    "        subset='validation',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    history = autoencoder.fit(\n",
    "        train_generator,\n",
    "        validation_data=test_generator,\n",
    "        steps_per_epoch=train_generator.samples // batch_size,\n",
    "        validation_steps=test_generator.samples // batch_size,\n",
    "        epochs=10,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Testing accuracy\n",
    "    print(\"Testing the autoencoder...\")\n",
    "    test_accuracy, precision, recall, f1 = test_autoencoder(autoencoder, test_generator)\n",
    "\n",
    "    # Plot training accuracy and testing accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.axhline(y=test_accuracy, color='r', linestyle='--', label='Testing Accuracy')\n",
    "    plt.title('Training and Testing Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
